# encoding: utf-8
import datetime
import pytz
import re
import threading
import random
import requests
import time
from web_ins import web_ins
import emoji
from apscheduler.schedulers.blocking import BlockingScheduler

def timestamp2day(timestramp):
    tz = pytz.timezone(pytz.country_timezones('cn')[0])
    format_local_time = datetime.datetime.fromtimestamp(timestramp, tz).strftime('%Y/%m/%d %H:%M:%S\t')
    return format_local_time

def get_news_list(news_para):
    log_string = 'Trying to get %s'%news_para
    print(log_string)
    my_s=requests.Session()
    my_s.headers.update({
        'Accept-Encoding': '',
        'Connection': 'keep-alive',
        'Content-Length': '0',
        'Host': 'api.iclient.ifeng.com',
        'User-Agent': 'Dalvik/2.1.0(Linux;U;Android 5.1;iphone249 plus 1 Build/LMY47I)',
        'Accept-Language':'zh-CN,zh;q=0.8'
    })
    my_data={}
    my_data['id']=news_para
    my_data['action'] = ''
    my_data['pullNum']='1'
    my_data['lastDoc'] = ',,,'
    my_data['gv'] = '5.7'
    my_data['av'] = '5.7'
    my_data['screen'] = '720x1280'
    my_data['nw'] = 'wifi'
    my_data['province'] = 'ä¸Šæµ·'
    my_data['city'] = 'ä¸Šæµ·'
    my_data['proid'] = 'ifengnews'
    try:
        r=my_s.get('http://api.iclient.ifeng.com/ClientNews',params=my_data,timeout=10)
    except Exception as e:
        print(e)
    else:
        return r

def log_in(receive_cookie):
    log_string = 'Trying to login...ifeng news'
    print(log_string)
    my_s=requests.Session()
    my_s.headers.update({
        'Accept-Encoding': '',
        'Connection': 'keep-alive',
        'Content-Length': '0',
        'Host': 'id.ifeng.com',
        'User-Agent': 'Dalvik/2.1.0(Linux;U;Android 5.1;iphone249 plus 1 Build/LMY47I)',
        'Accept-Language':'zh-CN,zh;q=0.8'
    })
    '''
    ä½“è‚²TY43 å†›äº‹JS83
    '''
    comment_url='http://icomment.ifeng.com/wappost.php'
    # comment_url='http://comment.ifeng.com/post.php'
    my_cookies={item['name']:item['value'] for item in receive_cookie }
    my_s.cookies.update(my_cookies)
    # r = my_s.get('http://id.ifeng.com', timeout=10)
    # res_txt = r.text
    # print(res_txt)
    # return
    # ''''KJ123,FOCUSKJ123''''
    news_data=[
        {'news_data': 'KJ123,FOCUSKJ123',
         'key_word': ' ',
         'reply_txt': ['æ¸…é™¤å¸‚åœºè´¥ç±»é…·éª‘å•è½¦ï¼Œæ¸…é€€ç”¨æˆ·æŠ¼é‡‘ï¼Œä¸¥æƒ©ä¸æ³•åˆ†å­ï¼Œè¿˜å…¬å¹³æ­£ä¹‰ç»™ç”¨æˆ·ã€‚',
                       'é…·éª‘å•è½¦ï¼Œå¸‚åœºè´¥ç±»']
         },
        {'news_data':'TY43,FOCUSTY43,TYTOPIC',
                'key_word':['ä¸­è¶…','ä¸­ç”²','ç”·è¶³','å›½è¶³','è¶³çƒ','è¶³å','è¶³å›','è¶³è”','è¶³æ€»','æ’å¤§','ä¸Šæ¸¯','æƒå¥','é²èƒ½','å›½å®‰',
              'å¯ŒåŠ›','äºšæ³°','å¼€æ–°','äº¿åˆ©','ç”³èŠ±','è¾½è¶³','äºšå† ','éƒæµ·ä¸œ'],
                'reply_txt':['åœæ­¢ç”·è¶³ğŸ’©èŒä¸šè”èµ›ï¼Œç”·è¶³ğŸ’©é€€å‡ºå›½é™…è¶³è”ï¼Œè§£æ•£ç”·è¶³å›½å®¶é˜ŸğŸ’©',
               'ä¸­å›½ç”·è¶³åœ¨ä¸–ç•Œæ¯å‡ºç°çš„å”¯ä¸€åŠæ³•ï¼šå’Œå—ææ´²ğŸ§åˆ†åœ¨ä¸€ç»„ï¼Œä¸»åœºå®šåœ¨æµ·å—ï¼Œå®¢åœºä¿å¹³ï¼Œä¸»åœºäº‰èƒœ,æœ€é‡è¦çš„è¦è¯·æµ·è±¹å½“è£åˆ¤ğŸ‘®',
               'ä¸­å›½ä¸é€‚åˆèŒä¸šè¶³çƒï¼Œé©¬ä¸Šè§£æ•£è¶³åç”·è¶³ï¼Œå¤šå»ºå‡ ä¸ªå…è´¹ç¯å…‰åœºè®©æ™®é€šäººç©ç©å°±è¡Œäº†',
               'åœåŠè¶³çƒï¼Œæ”¯æ´è¾¹ç–†ï¼›åœåŠè¶³çƒï¼Œå¤šå»ºç»¿åœ°ï¼›åœåŠè¶³çƒï¼Œå­¦ç”Ÿå‡è´Ÿï¼›åœåŠè¶³çƒï¼Œç¤¾ä¼šå°†æ›´å’Œè°ï¼Œå¤§å®¶ä¼šæ›´å¼€å¿ƒ',
               'è¶³åè¯·æŠŠä½ ä»¬å®šä½åœ¨æœåŠ¡äººå‘˜ä¸Šï¼Œè€Œä¸æ˜¯ç®¡ç†è€…ã€‚åˆ›é€ æ¡ä»¶è®©æ™®é€šçš„çƒ­çˆ±è¶³çƒè¿åŠ¨çš„äººä»¬èƒ½è½»æ¾çš„è¸¢åœºçƒï¼Œè®©å­©å­ä»¬èƒ½è½»æ¾çš„è¸¢åœºçƒï¼Œè¿™æ˜¯ä½ ä»¬æœ€è¯¥å¹²çš„',
               'æˆ‘ç°åœ¨å¹´è–ªäº”å…­ä¸‡ï¼ŒçœŸä¸å¤Ÿç”¨çš„ï¼Œå¬è¯´ç”·è¶³é˜Ÿå‘˜éƒ½æ˜¯å¹´è–ªåƒä¸‡ï¼Œæˆ‘ç°åœ¨ç”³è¯·è·³æ§½ï¼ŒåŠ å…¥å›½è¶³ï¼Œç†ç”±å¦‚ä¸‹ï¼š ä¸€ã€æˆ‘ä¹Ÿèƒ½ä¸èµ¢çƒï¼›äºŒã€ æˆ‘ä¹Ÿä¸èƒ½å¸¦çƒçªç ´å¯¹æ–¹çƒå‘˜ï¼›ä¸‰ã€å¯¹æ–¹çƒå‘˜å¸¦çƒä¹Ÿèƒ½å¾ˆè½»æ¾çš„æŠŠæˆ‘æ™ƒè¿‡ï¼›å››ã€é¢å¯¹å¯¹æ–¹çƒé—¨ï¼Œæˆ‘ä¹Ÿå°„ä¸è¿›çƒï¼›äº”ã€æˆ‘ä¹Ÿæ¥ä¸ä½é˜Ÿå‹ä¼ ç»™æˆ‘çš„çƒï¼›å…­ã€æˆ‘ä¹Ÿä¸èƒ½æŠŠçƒå‡†ç¡®åœ°ä¼ ç»™é˜Ÿå‹ï¼›ä¸ƒã€åœ¨åœºä¸Šï¼Œæˆ‘ä¹Ÿä¼šåƒå£é¦™ç³–ï¼›å…«ã€åœ¨åœºä¸Šæˆ‘ä¹Ÿæ•¢éª‚è£åˆ¤ï¼›ä¹ã€è¾“äº†çƒï¼Œé¢å¯¹åª’ä½“æˆ‘ä¹Ÿä¼šæ½¸ç„¶æ³ªä¸‹ï¼Œè¿‡ååˆåƒæ²¡äº‹äººä¸€æ ·ï¼›åã€æˆ‘ä¹Ÿçˆ±ç¾å¥³ç½‘çº¢å¥³æ¨¡ç‰¹ï¼Œæ³¡å§æ³¡åˆ°å¤§å¤©äº®ï¼›åä¸€ã€æˆ‘ä¹Ÿæœ‰çº¹èº«,ç»å¯¹æ¯”è´å…‹æ±‰å§†å¤šï¼›åäºŒã€æˆ‘ä¹Ÿå¯¹è±ªè½¦å¦‚æ•°å®¶ç,æœ‰é’±å°±ä¹°ã€‚åä¸‰ã€è¾“çƒäº†æˆ‘ä¹Ÿä¼šå‘å¾®åšï¼ŒæŒ‡è´£éª‚æˆ‘ä»¬çš„çƒè¿·',
               'æˆ‘ç°åœ¨å¹´è–ªäº”å…­ä¸‡ï¼ŒçœŸä¸å¤Ÿç”¨çš„ï¼Œå¬è¯´ç”·è¶³é˜Ÿå‘˜éƒ½æ˜¯å¹´è–ªåƒä¸‡ï¼Œæˆ‘ç°åœ¨ç”³è¯·è·³æ§½ï¼ŒåŠ å…¥å›½è¶³ï¼Œç†ç”±å¦‚ä¸‹ï¼š ä¸€ã€æˆ‘ä¹Ÿèƒ½ä¸èµ¢çƒï¼›äºŒã€ æˆ‘ä¹Ÿä¸èƒ½å¸¦çƒçªç ´å¯¹æ–¹çƒå‘˜ï¼›ä¸‰ã€å¯¹æ–¹çƒå‘˜å¸¦çƒä¹Ÿèƒ½å¾ˆè½»æ¾çš„æŠŠæˆ‘æ™ƒè¿‡ï¼›å››ã€é¢å¯¹å¯¹æ–¹çƒé—¨ï¼Œæˆ‘ä¹Ÿå°„ä¸è¿›çƒï¼›äº”ã€æˆ‘ä¹Ÿæ¥ä¸ä½é˜Ÿå‹ä¼ ç»™æˆ‘çš„çƒï¼›å…­ã€æˆ‘ä¹Ÿä¸èƒ½æŠŠçƒå‡†ç¡®åœ°ä¼ ç»™é˜Ÿå‹ï¼›ä¸ƒã€åœ¨åœºä¸Šï¼Œæˆ‘ä¹Ÿä¼šåƒå£é¦™ç³–ï¼›å…«ã€åœ¨åœºä¸Šæˆ‘ä¹Ÿæ•¢éª‚è£åˆ¤ï¼›ä¹ã€è¾“äº†çƒï¼Œé¢å¯¹åª’ä½“æˆ‘ä¹Ÿä¼šæ½¸ç„¶æ³ªä¸‹ï¼Œè¿‡ååˆåƒæ²¡äº‹äººä¸€æ ·ï¼›åã€æˆ‘ä¹Ÿçˆ±ç¾å¥³ç½‘çº¢å¥³æ¨¡ç‰¹ï¼Œæ³¡å§æ³¡åˆ°å¤§å¤©äº®ï¼›åä¸€ã€æˆ‘ä¹Ÿæœ‰çº¹èº«,ç»å¯¹æ¯”è´å…‹æ±‰å§†å¤šï¼›åäºŒã€æˆ‘ä¹Ÿå¯¹è±ªè½¦å¦‚æ•°å®¶ç,æœ‰é’±å°±ä¹°ã€‚åä¸‰ã€è¾“çƒäº†æˆ‘ä¹Ÿä¼šå‘å¾®åšï¼ŒæŒ‡è´£éª‚æˆ‘ä»¬çš„çƒè¿·',
               ]},]
    my_s.headers.update({
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Host': 'comment.ifeng.com',
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:56.0) Gecko/20100101 Firefox/56.0',
        'Accept-Language':'zh-CN,zh;q=0.8',
    })
    comment_ext2 = {'comment_level': '0',
                    'device_type': 'iphone249 plus 1',
                    'from': 'sh',
                    'isSync': '1',
                    'isTrends': '0',
                    'lat': '39.970585',
                    'location': 'ä¸¹ä¸œé¸­ç»¿æ±Ÿ',
                    'lon': '123.975259',
                    'nickname': 'xxxxx',
                    'userimg': 'http://my.ifengimg.com/2017/08/02/8326ef78edbd78cf1501652140_1.jpg'
                    }
    try:
        emoji_res = requests.get('https://api.github.com/emojis').json()
    except:
        print('è·å–emojiå¤±è´¥')
        return
    for news_item in news_data:
        r=get_news_list(news_item['news_data'])
        res=r.json()
        try:
            find_item=res[0]['item']
        except:
            print('æœªè·å–åˆ°%såˆ—è¡¨'%(news_item['news_data']))
            return
        for item in find_item:
            try:
                current_url=item['commentsUrl']
            except:
                print('æ­¤æ¡æ— å¯ç”¨ä¿¡æ¯')
                continue
            if bool(re.match(r'sub_\d*?',current_url)) and \
                    bool(([word for word in news_item['key_word'] if word in item['title']]) or news_item['key_word']==''):
                global used_url_list
                global last_refresh_time
                if int(time.time()-last_refresh_time)>24*60*60:
                    used_url_list.clear()
                    last_refresh_time=int(time.time())
                    print('æ¸…ç©ºurl_list')
                if not item['commentsUrl'] in used_url_list:
                    used_url_list.append(item['commentsUrl'])
                    lead_emoji_list = ['ğŸ‘', 'ğŸ’©', 'ğŸ‘®']
                    while (True):
                        if len(lead_emoji_list) > 0:
                            my_data = {}
                            my_data['docName'] = item['title']
                            my_data['docUrl'] = item['commentsUrl']
                            tail_emoji=lead_emoji_list.pop(random.randrange(0,len(lead_emoji_list)))+''.join([re.sub(r":.*?:",'',
                                             emoji.emojize(':%s:'%random.choice(list(emoji_res))))for i in range(5)])
                            reply_message_index=random.randrange(0,len(news_item['reply_txt']))
                            my_data['content'] = news_item['reply_txt'][reply_message_index]+tail_emoji
                            comment_ext2['docId'] = item['commentsUrl']
                            my_data['ext2'] = str(comment_ext2)
                            try:
                                r = my_s.get(comment_url, params=my_data,timeout=10)
                                if bool(r):
                                    res_txt=r.json()
                                    print('news_%då‘å¸ƒäº:%sæ ‡é¢˜:%s\nçŠ¶æ€:%sæç¤º:%så›å¤:%s' % (len(used_url_list),
                                    item['updateTime'], item['title'], res_txt['code'],res_txt['message'] if 'message' in res_txt.keys()else ' ',(str(reply_message_index)+tail_emoji)))
                                else:
                                    print('news_%då‘å¸ƒäº:%sæ ‡é¢˜:%s\nçŠ¶æ€:%sæç¤º:%så›å¤:%s' % (len(used_url_list),item['updateTime'], item['title'], 'å¤±è´¥',' ',(str(reply_message_index)+tail_emoji)))
                                    break
                            except Exception as e:
                                print(e)
                                return
                            time.sleep(10)
                        else:
                            break
                else:
                    res_txt='å·²å‘è¿‡è¯„è®º'
                    print(
                        'news_%då‘å¸ƒäº:%sæ ‡é¢˜:%s\nçŠ¶æ€:%s' % (len(used_url_list),
                                                          item['updateTime'], item['title'], res_txt))
                time.sleep(5)
        print('news_å®ŒæˆæŸ¥æ‰¾æ—¶é—´ï¼š%s'%timestamp2day(time.time()))

used_url_list=[]
last_refresh_time=int(time.time())
USER_NAME='xxx'
PASS_WORD='xxx'
my_web=web_ins(USER_NAME,PASS_WORD)
cookie=my_web.log_in()
log_in(cookie)
sched = BlockingScheduler(timezone='Asia/Shanghai')
def new_scheduled():
    @sched.scheduled_job('interval', id='ifeng_news',  minutes=15)
    def scheduled_ins_down():
        time.sleep(1)
        log_in(cookie)
    sched.start()
t = threading.Thread(target =new_scheduled)
t.start()
